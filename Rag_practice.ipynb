{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "xTTkxVJHvhwG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q youtube-transcript-api langchain_community langchain-openai tiktoken python_dotenv langchain chromadb langchain_core langchain_text_splitters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elOxwqa4viY8",
        "outputId": "09d320c4-776e-48d8-be63-724ccd168560"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m99.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m104.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-adk 1.21.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.39.1 which is incompatible.\n",
            "google-adk 1.21.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import ChatOpenAI,OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "BzP7VTuCvsmT"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# I did this because the youtube_transcript_api was giving a blockage error\n",
        "\n",
        "transcript = \"imagine you happen across a short movie script that describes a scene between a person and their AI assistant the script has what the person asks the AI but the ai's response has been torn off suppose you also have this powerful magical machine that can take any text and provide a sensible prediction of what word comes next you could then finish the script by feeding in what you have to the machine seeing what it would predict to start the ai's answer and then repeating this over and over with a growing script completing the dialogue when you interact with a chatbot this is exactly what's happening a large language model is a sophisticated mathematical function that predicts what word comes next for any piece of text instead of predicting one word with certainty though what it does is assign a probability to all possible next words to build a chatbot what you do is lay out some text that describes an interaction between a user and a hypothetical AI assistant you add on whatever the user types in as the first part of that interaction and then you have the model repeatedly predict the next word that such a hypothetical AI assistant would say in response and that's what's presented to the user in doing this the output tends to look a lot more natural if you allow it to select less likely words along the way at random so what this means is even though the model itself is deterministic a given prompt typically gives a different answer each time it's run models learn how to make these predictions by processing an enormous amount of text typically pulled from the internet for a standard human to read the amount of text that was used to train gpt3 for example if they read non-stop 24/7 it would take over 2600 years larger models since then train on much much more you can think of training a little bit like tuning the dials on a big machine the way that a language model behaves is entirely determined by these many different continuous values usually called parameters or weights changing those parameters will change the probabilities that the model gives for the next word on a given input what puts the large inlarge language model is how they can have hundreds of billions of these parameters no human ever deliberately sets those parameters instead they begin at random meaning the model just outputs gibberish but they're repeatedly refined based on many example pieces of text one of these training examples could be just a handful of words or it could be thousands but in either case the way this works is to pass in all but the last word from that example into the model and compare the prediction that it makes with the true last word from the example an algorithm called back propagation is used to tweak all of the parameters in such a way that it makes the model a little more likely to choose the true last word and a little less likely to choose all the others when you do this for many many trillions of examples not only does the model start to give more accurate predictions on the training data but it also starts to make more reasonable predictions on text that it's never seen before given the huge number of parameters and the enormous amount of training data the scale of computation involved in training a large language model is mindboggling to illustrate imagine that you could perform 1 billion additions and multiplications every single second how long do you think that it would take for you to do all of the operations involved in training the largest language models do you think it would take a year maybe something like 10,000 years the answer is actually much more than that it's well over 100 million years this is only part of the story though this whole process is called pre-training the goal of autocompleting a random passage of text from the Internet is very different from the goal of being a good AI assistant to address this chatbots undergo another type of training just as important called reinforcement learning with human feedback workers flag unhelpful or problematic predictions and their Corrections further change the model's parameters making them more likely to give predictions that users prefer looking back at the pre-training though this staggering amount of computation is only made possible by using special computer chips that are optimized for running many many operations in parallel known as gpus however not all language models models can be easily parallelized prior to 2017 most language models would process text one word at a time but then a team of researchers at Google introduced a new model known as the Transformer Transformers don't read text from the start to the Finish they soak it all in at once in parallel the very first step inside a Transformer and most other language models for that matter is to associate each word with a long list of numbers the reason for this is that the training process only works with continuous values so you have to somehow encode language using numbers and each of these list of numbers May somehow encode the meaning of the corresponding word what makes Transformers unique is their Reliance on a special operation known as attention this operation gives all of these lists of numbers a chance to talk to one another and refine the meanings that they encode based on the context around all done in parallel for example the numbers encoding the word Bank might be changed based on the context surrounding it to somehow encode the more specific notion of a river bank Transformers typically also include a second type of operation known as a feedforward neural network and this gives the model extra capacity to store more patterns about language learned during training all of this data repeatedly flows through many different iterations of these two fundamental operations and as it does so the hope is that each list of numbers is enriched to code whatever information might be needed to make an accurate prediction of what word follows in the passage at the end one final function is performed on the last Vector in this sequence which now has had a chance to be influenced by all the other context from the input text as well as everything the model learned during training to produce a prediction of the next word again the model's prediction looks like a probability for every possible next word although researchers design the framework for how each of these steps work it's important to understand that the specific behavior is an emergent phenomenon based on how those hundreds of billions of parameters are tuned during training this makes it incredibly challenging to determine why the model makes the exact predictions that it does what you can see is that when you use large language model predictions to autocomplete a prompt the words that it generates are uncannily fluent fascinating and even useful if you're a new viewer and you're curious about more details on how Transformers and attention work boy do I have some material for you one option is to jump into a series I made about deep learning where we visualize and motivate the details of attention and all the other steps in a Transformer but also on my second Channel I just posted a talk that I gave a couple months ago about this topic for the company TNG in Munich sometimes I actually prefer the content that I make as a casual talk rather than a produced video but I leave it up to you which one of these feels like the better follow on\""
      ],
      "metadata": {
        "id": "Rz8_GRVrwSWn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splitter = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
        "chunks = splitter.create_documents([transcript])"
      ],
      "metadata": {
        "id": "8GQI19-0wVTk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2yLlM7gwkwQ",
        "outputId": "d877693b-1409-4df0-a5b4-e5a1a772efae"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chunks[3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ja-c0F87wpnk",
        "outputId": "0cb52486-6e9d-4f19-99c2-7f6751b7954e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={}, page_content=\"one of these training examples could be just a handful of words or it could be thousands but in either case the way this works is to pass in all but the last word from that example into the model and compare the prediction that it makes with the true last word from the example an algorithm called back propagation is used to tweak all of the parameters in such a way that it makes the model a little more likely to choose the true last word and a little less likely to choose all the others when you do this for many many trillions of examples not only does the model start to give more accurate predictions on the training data but it also starts to make more reasonable predictions on text that it's never seen before given the huge number of parameters and the enormous amount of training data the scale of computation involved in training a large language model is mindboggling to illustrate imagine that you could perform 1 billion additions and multiplications every single second how long do\")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = OpenAIEmbeddings(model='text-embedding-3-small')\n",
        "vector_store = Chroma.from_documents(chunks,embeddings)"
      ],
      "metadata": {
        "id": "eUyZA6DiwrWw"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_store"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtZXtgYNw9S0",
        "outputId": "d393ad8f-6ae7-45df-e730-f707ff370cf4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langchain_community.vectorstores.chroma.Chroma at 0x7f6c5e58f8f0>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vector_store._collection.get()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Px8E2_JxWWD",
        "outputId": "06e31b6a-92e7-4f5f-d812-4d417ae3c946"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ids': ['62451d8e-7182-417c-b75c-de413293e3a7',\n",
              "  '5ecb2d11-5b2b-479d-81d2-73ba179e695f',\n",
              "  '2c611b92-065d-4e13-b8ff-6b2f8322b9d6',\n",
              "  '6deeaea4-4e43-418a-88ca-c71190729fa3',\n",
              "  'd5d005c4-5796-41b1-be85-45159577337a',\n",
              "  'c55136b0-eeb6-4f8e-9113-9ac6525265fc',\n",
              "  '76662b51-b289-4e6f-97e3-9d716486548b',\n",
              "  '003f3ded-e1ca-4c53-865c-0a8a52e955dc',\n",
              "  '7068ec77-cfdf-4b7b-ad93-51cd9d740124',\n",
              "  '494c391f-acd0-4632-9a52-25128ce2cd1e'],\n",
              " 'embeddings': None,\n",
              " 'documents': [\"imagine you happen across a short movie script that describes a scene between a person and their AI assistant the script has what the person asks the AI but the ai's response has been torn off suppose you also have this powerful magical machine that can take any text and provide a sensible prediction of what word comes next you could then finish the script by feeding in what you have to the machine seeing what it would predict to start the ai's answer and then repeating this over and over with a growing script completing the dialogue when you interact with a chatbot this is exactly what's happening a large language model is a sophisticated mathematical function that predicts what word comes next for any piece of text instead of predicting one word with certainty though what it does is assign a probability to all possible next words to build a chatbot what you do is lay out some text that describes an interaction between a user and a hypothetical AI assistant you add on whatever the\",\n",
              "  \"a probability to all possible next words to build a chatbot what you do is lay out some text that describes an interaction between a user and a hypothetical AI assistant you add on whatever the user types in as the first part of that interaction and then you have the model repeatedly predict the next word that such a hypothetical AI assistant would say in response and that's what's presented to the user in doing this the output tends to look a lot more natural if you allow it to select less likely words along the way at random so what this means is even though the model itself is deterministic a given prompt typically gives a different answer each time it's run models learn how to make these predictions by processing an enormous amount of text typically pulled from the internet for a standard human to read the amount of text that was used to train gpt3 for example if they read non-stop 24/7 it would take over 2600 years larger models since then train on much much more you can think of\",\n",
              "  \"human to read the amount of text that was used to train gpt3 for example if they read non-stop 24/7 it would take over 2600 years larger models since then train on much much more you can think of training a little bit like tuning the dials on a big machine the way that a language model behaves is entirely determined by these many different continuous values usually called parameters or weights changing those parameters will change the probabilities that the model gives for the next word on a given input what puts the large inlarge language model is how they can have hundreds of billions of these parameters no human ever deliberately sets those parameters instead they begin at random meaning the model just outputs gibberish but they're repeatedly refined based on many example pieces of text one of these training examples could be just a handful of words or it could be thousands but in either case the way this works is to pass in all but the last word from that example into the model\",\n",
              "  \"one of these training examples could be just a handful of words or it could be thousands but in either case the way this works is to pass in all but the last word from that example into the model and compare the prediction that it makes with the true last word from the example an algorithm called back propagation is used to tweak all of the parameters in such a way that it makes the model a little more likely to choose the true last word and a little less likely to choose all the others when you do this for many many trillions of examples not only does the model start to give more accurate predictions on the training data but it also starts to make more reasonable predictions on text that it's never seen before given the huge number of parameters and the enormous amount of training data the scale of computation involved in training a large language model is mindboggling to illustrate imagine that you could perform 1 billion additions and multiplications every single second how long do\",\n",
              "  \"scale of computation involved in training a large language model is mindboggling to illustrate imagine that you could perform 1 billion additions and multiplications every single second how long do you think that it would take for you to do all of the operations involved in training the largest language models do you think it would take a year maybe something like 10,000 years the answer is actually much more than that it's well over 100 million years this is only part of the story though this whole process is called pre-training the goal of autocompleting a random passage of text from the Internet is very different from the goal of being a good AI assistant to address this chatbots undergo another type of training just as important called reinforcement learning with human feedback workers flag unhelpful or problematic predictions and their Corrections further change the model's parameters making them more likely to give predictions that users prefer looking back at the pre-training\",\n",
              "  \"flag unhelpful or problematic predictions and their Corrections further change the model's parameters making them more likely to give predictions that users prefer looking back at the pre-training though this staggering amount of computation is only made possible by using special computer chips that are optimized for running many many operations in parallel known as gpus however not all language models models can be easily parallelized prior to 2017 most language models would process text one word at a time but then a team of researchers at Google introduced a new model known as the Transformer Transformers don't read text from the start to the Finish they soak it all in at once in parallel the very first step inside a Transformer and most other language models for that matter is to associate each word with a long list of numbers the reason for this is that the training process only works with continuous values so you have to somehow encode language using numbers and each of these\",\n",
              "  'each word with a long list of numbers the reason for this is that the training process only works with continuous values so you have to somehow encode language using numbers and each of these list of numbers May somehow encode the meaning of the corresponding word what makes Transformers unique is their Reliance on a special operation known as attention this operation gives all of these lists of numbers a chance to talk to one another and refine the meanings that they encode based on the context around all done in parallel for example the numbers encoding the word Bank might be changed based on the context surrounding it to somehow encode the more specific notion of a river bank Transformers typically also include a second type of operation known as a feedforward neural network and this gives the model extra capacity to store more patterns about language learned during training all of this data repeatedly flows through many different iterations of these two fundamental operations and',\n",
              "  \"the model extra capacity to store more patterns about language learned during training all of this data repeatedly flows through many different iterations of these two fundamental operations and as it does so the hope is that each list of numbers is enriched to code whatever information might be needed to make an accurate prediction of what word follows in the passage at the end one final function is performed on the last Vector in this sequence which now has had a chance to be influenced by all the other context from the input text as well as everything the model learned during training to produce a prediction of the next word again the model's prediction looks like a probability for every possible next word although researchers design the framework for how each of these steps work it's important to understand that the specific behavior is an emergent phenomenon based on how those hundreds of billions of parameters are tuned during training this makes it incredibly challenging to\",\n",
              "  \"important to understand that the specific behavior is an emergent phenomenon based on how those hundreds of billions of parameters are tuned during training this makes it incredibly challenging to determine why the model makes the exact predictions that it does what you can see is that when you use large language model predictions to autocomplete a prompt the words that it generates are uncannily fluent fascinating and even useful if you're a new viewer and you're curious about more details on how Transformers and attention work boy do I have some material for you one option is to jump into a series I made about deep learning where we visualize and motivate the details of attention and all the other steps in a Transformer but also on my second Channel I just posted a talk that I gave a couple months ago about this topic for the company TNG in Munich sometimes I actually prefer the content that I make as a casual talk rather than a produced video but I leave it up to you which one of\",\n",
              "  'months ago about this topic for the company TNG in Munich sometimes I actually prefer the content that I make as a casual talk rather than a produced video but I leave it up to you which one of these feels like the better follow on'],\n",
              " 'uris': None,\n",
              " 'included': ['metadatas', 'documents'],\n",
              " 'data': None,\n",
              " 'metadatas': [None, None, None, None, None, None, None, None, None, None]}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vector_store.as_retriever(search_type='similarity',search_kwargs={'k':4})"
      ],
      "metadata": {
        "id": "Fa-cnzeGxljN"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "im7ASkxwygQm",
        "outputId": "0ebeb6fb-9e09-4652-9a87-aa80e018f227"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x7f6c5e58f8f0>, search_kwargs={'k': 4})"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriever.invoke('What is ai')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkd6hfbfyjZ7",
        "outputId": "7e7e4af0-3cb4-4446-a5ac-fc247bd1bc3f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={}, page_content=\"imagine you happen across a short movie script that describes a scene between a person and their AI assistant the script has what the person asks the AI but the ai's response has been torn off suppose you also have this powerful magical machine that can take any text and provide a sensible prediction of what word comes next you could then finish the script by feeding in what you have to the machine seeing what it would predict to start the ai's answer and then repeating this over and over with a growing script completing the dialogue when you interact with a chatbot this is exactly what's happening a large language model is a sophisticated mathematical function that predicts what word comes next for any piece of text instead of predicting one word with certainty though what it does is assign a probability to all possible next words to build a chatbot what you do is lay out some text that describes an interaction between a user and a hypothetical AI assistant you add on whatever the\"),\n",
              " Document(metadata={}, page_content=\"a probability to all possible next words to build a chatbot what you do is lay out some text that describes an interaction between a user and a hypothetical AI assistant you add on whatever the user types in as the first part of that interaction and then you have the model repeatedly predict the next word that such a hypothetical AI assistant would say in response and that's what's presented to the user in doing this the output tends to look a lot more natural if you allow it to select less likely words along the way at random so what this means is even though the model itself is deterministic a given prompt typically gives a different answer each time it's run models learn how to make these predictions by processing an enormous amount of text typically pulled from the internet for a standard human to read the amount of text that was used to train gpt3 for example if they read non-stop 24/7 it would take over 2600 years larger models since then train on much much more you can think of\"),\n",
              " Document(metadata={}, page_content=\"scale of computation involved in training a large language model is mindboggling to illustrate imagine that you could perform 1 billion additions and multiplications every single second how long do you think that it would take for you to do all of the operations involved in training the largest language models do you think it would take a year maybe something like 10,000 years the answer is actually much more than that it's well over 100 million years this is only part of the story though this whole process is called pre-training the goal of autocompleting a random passage of text from the Internet is very different from the goal of being a good AI assistant to address this chatbots undergo another type of training just as important called reinforcement learning with human feedback workers flag unhelpful or problematic predictions and their Corrections further change the model's parameters making them more likely to give predictions that users prefer looking back at the pre-training\"),\n",
              " Document(metadata={}, page_content='each word with a long list of numbers the reason for this is that the training process only works with continuous values so you have to somehow encode language using numbers and each of these list of numbers May somehow encode the meaning of the corresponding word what makes Transformers unique is their Reliance on a special operation known as attention this operation gives all of these lists of numbers a chance to talk to one another and refine the meanings that they encode based on the context around all done in parallel for example the numbers encoding the word Bank might be changed based on the context surrounding it to somehow encode the more specific notion of a river bank Transformers typically also include a second type of operation known as a feedforward neural network and this gives the model extra capacity to store more patterns about language learned during training all of this data repeatedly flows through many different iterations of these two fundamental operations and')]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model='gpt-4o-mini',temperature=0.3)"
      ],
      "metadata": {
        "id": "jrNakvglyqs4"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate(\n",
        "    template=\"\"\"\n",
        "      You are a helpful assistant.\n",
        "      Answer ONLY from the provided transcript context.\n",
        "      If the context is insufficient, just say you don't know.\n",
        "\n",
        "      {context}\n",
        "      Question: {question}\n",
        "    \"\"\",\n",
        "    input_variables = ['context', 'question']\n",
        ")"
      ],
      "metadata": {
        "id": "TuYtfj1vy6Xo"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CHain\n",
        "\n",
        "from langchain_core.runnables import RunnableParallel,RunnablePassthrough,RunnableLambda\n",
        "from langchain_core.output_parsers import StrOutputParser"
      ],
      "metadata": {
        "id": "GXY-jKfUy8kw"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_docs(retrieved_docs):\n",
        "  context_text = '\\n\\n'.join(doc.page_content for doc in retrieved_docs)\n",
        "  return context_text"
      ],
      "metadata": {
        "id": "rOz66FfRzSfy"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parallel_chain = RunnableParallel({\n",
        "    'context': retriever | RunnableLambda(format_docs),\n",
        "    'question': RunnablePassthrough()\n",
        "})"
      ],
      "metadata": {
        "id": "uTxNqyyqzqr-"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parallel_chain.invoke('What is AI')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNCNQA120fxS",
        "outputId": "5592a512-ae97-4146-cd45-b568232ffe38"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'context': \"imagine you happen across a short movie script that describes a scene between a person and their AI assistant the script has what the person asks the AI but the ai's response has been torn off suppose you also have this powerful magical machine that can take any text and provide a sensible prediction of what word comes next you could then finish the script by feeding in what you have to the machine seeing what it would predict to start the ai's answer and then repeating this over and over with a growing script completing the dialogue when you interact with a chatbot this is exactly what's happening a large language model is a sophisticated mathematical function that predicts what word comes next for any piece of text instead of predicting one word with certainty though what it does is assign a probability to all possible next words to build a chatbot what you do is lay out some text that describes an interaction between a user and a hypothetical AI assistant you add on whatever the\\n\\na probability to all possible next words to build a chatbot what you do is lay out some text that describes an interaction between a user and a hypothetical AI assistant you add on whatever the user types in as the first part of that interaction and then you have the model repeatedly predict the next word that such a hypothetical AI assistant would say in response and that's what's presented to the user in doing this the output tends to look a lot more natural if you allow it to select less likely words along the way at random so what this means is even though the model itself is deterministic a given prompt typically gives a different answer each time it's run models learn how to make these predictions by processing an enormous amount of text typically pulled from the internet for a standard human to read the amount of text that was used to train gpt3 for example if they read non-stop 24/7 it would take over 2600 years larger models since then train on much much more you can think of\\n\\nscale of computation involved in training a large language model is mindboggling to illustrate imagine that you could perform 1 billion additions and multiplications every single second how long do you think that it would take for you to do all of the operations involved in training the largest language models do you think it would take a year maybe something like 10,000 years the answer is actually much more than that it's well over 100 million years this is only part of the story though this whole process is called pre-training the goal of autocompleting a random passage of text from the Internet is very different from the goal of being a good AI assistant to address this chatbots undergo another type of training just as important called reinforcement learning with human feedback workers flag unhelpful or problematic predictions and their Corrections further change the model's parameters making them more likely to give predictions that users prefer looking back at the pre-training\\n\\nimportant to understand that the specific behavior is an emergent phenomenon based on how those hundreds of billions of parameters are tuned during training this makes it incredibly challenging to determine why the model makes the exact predictions that it does what you can see is that when you use large language model predictions to autocomplete a prompt the words that it generates are uncannily fluent fascinating and even useful if you're a new viewer and you're curious about more details on how Transformers and attention work boy do I have some material for you one option is to jump into a series I made about deep learning where we visualize and motivate the details of attention and all the other steps in a Transformer but also on my second Channel I just posted a talk that I gave a couple months ago about this topic for the company TNG in Munich sometimes I actually prefer the content that I make as a casual talk rather than a produced video but I leave it up to you which one of\",\n",
              " 'question': 'What is AI'}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parser = StrOutputParser()"
      ],
      "metadata": {
        "id": "8XrL3LKL0jcr"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_chain = parallel_chain | prompt | llm | parser"
      ],
      "metadata": {
        "id": "-32FCwPu09Sx"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_chain.invoke('Summarize the text')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "WJ2JxRZE1Iza",
        "outputId": "417d47c1-497e-40e9-8b56-48e4ad1a2da9"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The text explains how large language models, like chatbots, predict the next word in a sequence by using a sophisticated mathematical function that assigns probabilities to possible next words. It describes the training process, which involves encoding language into numerical representations and refining these meanings through operations like attention and feedforward neural networks. The emergent behavior of the model is influenced by the tuning of its parameters during training, making it challenging to understand why specific predictions are made. The text also mentions resources for further learning about Transformers and attention mechanisms in deep learning.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zMWJ6qka1TZC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}